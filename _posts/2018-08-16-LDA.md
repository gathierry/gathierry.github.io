---
layout: post
title: 用 LDA 降维
categories: [data science]
tags: [data analysis, machine learning]
description: LDA 是一种常用的监督降维方法。
---

LDA 是一种常用的**监督**降维方法。LDA 在降维时注意保留类间距和类内距的信息。  
假设有 $$N$$ 组训练数据，其中第一类 $$n_1$$ 个，第二类 $$n_2$$ 个，...，第 K 类 $$n_K$$ 个，特征数是 $$m$$。

- Within-class scatter matrix $$S_W$$

$$ S_W = \sum_{i=1}^K S_i \\
S_i = (N_i - 1)\Sigma_i = \sum_{x\in Class_i}(x-\mu_i)(x-\mu_i)^\top $$

- Between-class scatter matrix $$S_B$$

$$ S_B = \sum_{i=1}^K N_i (\mu_i-\mu_{global})(\mu_i-\mu_{global})^\top$$

- Global scatter matrix $$S_T$$

$$ S_T=\sum_x (x-\mu)(x-\mu)^\top $$

$$S^T = S^B+S^W $$

定义变换后 $$y=w^\top x$$，$$w$$ 是一个 $$m\times d$$ 的矩阵（假设要降到 d 维）

$$\widetilde{S}_W = w^\top S_W w \\
\widetilde{S}_B = w^\top S_B w$$

LDA 的目标是优化 

$$\max_{w}\frac{tr(w^\top S_B w)}{tr(w^\top S_W w)} \\
= \max_{w} \sum_{i=1}^d \frac{w_i^\top S_B w_i}{w_i^\top S_W w_i}$$  

因为对矩阵优化比较困难，所以对每个元素分别优化  
令 $$||w_i^\top S_W w_i||=1$$，问题转化成

$$
\min_w -w_i^\top S_B w_i \\
s.t. ||w_i^\top S_W w_i||=1
$$

拉格朗日法求解

$$
\begin{align}
L &=  -w_i^\top S_B w_i + \lambda (||w_i^\top S_W w_i||-1) \\
\frac{\partial L}{\partial w_i} &= -2S_B w_i + 2\lambda S_W w_i = 0 \\
S_B w_i &= \lambda S_W w_i \\
S_W^{-1}S_Bw_i &= \lambda w_i
\end{align}
$$

所以 $$S_W^{-1}S_B$$ 的特征向量就是我们要求的 $$w_i$$。根据特征值取最大的 d 个特征值所对应的特征向量。
