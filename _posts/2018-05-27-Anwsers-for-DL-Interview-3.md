---
layout: post
title: 深度学习面试你必须知道这些答案（三）
categories: [data science]
tags: [deep learning]
description: 本文节选了网上分享的一部分深度学习相关的面试题，并将搜集到的答案随之附上。
---
本文节选了网上分享的一部分深度学习相关的面试题，并将搜集到的答案随之附上。  
参考了来自 _https://cloud.tencent.com/developer/article/1064505_ 的内容。

#### Q21. MLP 的万能近似性质
万能近似定理 (universal approximation theorem)(Hornik et al., 1989; Cybenko, 1989) 表明

- 一个前馈神经网络如果具有线性输出层和至少一层具有任何一种 “挤压” 性质的激活函数(e.g. sigmoid) 的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的 Borel 可测函数。
- 前馈网络的导数也可以任意好地来近似函数的导数 (Hornik et al., 1990)。
- 万能近似定理也已经被证明对于更广泛类别的激活函数也是适用的，其中就包括现在常用的整流线性单元 (Leshno et al., 1993)。
- 万能近似定理说明了，存在一个足够大的网络能够达到我们所希望的任意精度，但是定理并没有说这个网络有多大，可能大的不可实现。

*Borel 可测的概念超出了本书的范畴；对于我们想要实现的目标，只需要知道定义在 $$\mathbb{R}^n$$ 的有界闭集上的任意连续函数是 Borel 可测的， 因此可以用神经网络来近似。神经网络也可以近似从任何有限维离散空间映射到另一个的任意函数。*

#### Q22. 在前馈网络中，深度与宽度的关系及表示能力的差异
更深的模型往往表现更好。这不仅仅是因为模型更大。Goodfellow et al. (2014d) 的实验表明，增加卷积网络层中参数的数量，但是不增加它们的深度，在提升测试集性能方面几乎没有效果。  
浅层模型在参数数量达到 2000 万时就过拟合，而深层模型在参数数量超过 6000 万时仍然表现良好。这表明，使用深层模型表达出了对模型可以学习的函数空间的有用偏好。具体来说，它表达了一种信念，即该函数应该由许多更简单的函数复合在一起而得到。这可能导致学习由更简单的表示所组成的表示或者学习具有顺序依赖步骤的程序。

#### Q23. 为什么交叉熵损失可以提高具有 sigmoid 和 softmax 输出的模型的性能，而使用均方误差损失则会存在很多问题。分段线性隐藏层代替 sigmoid 的利弊
- 均方误差在 20 世纪 80 年代和 90 年代流行，但逐渐被交叉熵损失替代，并且最大似然原 理的想法在统计学界和机器学习界之间广泛传播。使用交叉熵损失大大提高了具有 sigmoid 和 softmax 输出的模型的性能，而当使用均方误差损失时会存在饱和和学习缓慢的问题。
- 对于小的数据集，Jarrett et al. (2009b) 观察到，使用整流非线性甚至比学习隐藏层的权重值更加重要。随机的权重足以通过整流网络传播有用的信息，允许在顶部的分类器层学习如何将不同的特征向量映射到类标识。  
当有更多数据可用时，学习开始提取足够的有用知识来超越随机选择参数的性能。Glorot et al. (2011a) 说明，__在深度整流网络中的学习比在激活函数具有曲率或两侧饱和的深度网络中的学习更容易__。  
整流线性单元还表明神经科学继续对深度学习算法的发展产生影响。Glorot et al. (2011a) 从生物学考虑整流线性单元的导出。半整流非线性旨在描述生物神经元的这些性质:
  1. 对于某些输入，生物神经元是完全不活跃的。
  2. 对于某些输入，生物神经元的输出和它的输入成比例。
  3. 大多数时间， 生物神经元是在它们不活跃的状态下进行操作，即稀疏激活。

#### Q24. 表示学习的发展的初衷？并介绍其典型例子：自编码器
- 对于许多任务来说，我们很难知道应该提取哪些特征，解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为**表示学习**。
- 表示学习算法的典型例子是自编码器。自编码器由一个编码器函数和一个解码器函数组合而成。编码器函数将输入数据转换为一种不同的表示，而解码器函数则将这个新的表示转换到原来的形式。我们期望当输入数据经过编码器和解码器之后尽可能多地保留信息，同时希望新的表示有各种好的特性，这也是自编码器的训练目标。

#### Q25. 在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚
我们通常只对权重做惩罚而不对偏置做正则惩罚。

- 精确拟合偏置所需的数据通常比拟合权重少得多。
- 每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。
- **正则化偏置参数可能会导致明显的欠拟合**。

#### Q26. 在深度学习神经网络中，所有的层中考虑使用相同的权重衰减的利弊
有时希望对网络的每一层使用单独的惩罚，并分配不同的系数。但是寻找合适的多个超参数的代价很大，因此为了减少搜索空间，我们会在所有层使用相同的权重衰减。

#### Q27. 正则化过程中，权重衰减与 Hessian 矩阵中特征值的一些关系，以及与梯度弥散，梯度爆炸的关系
令 $$w^*$$ 为未正则化的目标函数取得最小训练误差时的权重向量，即 $$w^*=\arg\min_wJ(w)$$，在 $$w^*$$ 的邻域对目标函数做二次近似。

$$
\hat{J}(\theta)=J(w^*)+\frac{1}{2}(w-w^*)^\top H(w-w^*)
$$

因为 $$w^*$$ 是 $$J$$ 的一个最优点，此时一阶项梯度 $$H(w-w^*)$$ 为零，而 $$H$$ 是半正定的。  
如果加入正则项，梯度变为
$$
\alpha w + H(w-w^*) = 0 \\
w = (H+\alpha I)^{-1}Hw^*
$$
当 $$\alpha$$ 趋于 0 时，$$w$$ 会趋向 $$w^*$$。
将 $$H$$ 分解成对角矩阵 $$\Lambda$$ 和一组特征向量的标准正交基 $$Q$$，上式化为
$$
\begin{align}
w & = (Q\Lambda Q^\top + \alpha I)^{-1}Q \Lambda Q^\top w^* \\
  & = (Q(\Lambda+\alpha I)Q^\top)^{-1}Q\Lambda Q^\top w^* \\
  & = Q(\Lambda+\alpha I))^{-1}\Lambda Q^\top w^*
\end{align}
$$
随着 $$\alpha$$ 增加，我们会根据 $$\frac{\lambda_i}{\lambda_i+\alpha}$$ 因子缩放与 H 第 i 个特征向量对齐的 $$w^*$$ 的分量。沿着特征值较大的方向（$$\lambda_i >> \alpha$$）正则化的影响较小。而 $$\lambda_i << \alpha$$ 的分量将会收缩到几乎为零。
<img src="/images/2018-05-27-Anwsers-for-DL-Interview-3/wdecay.png" width="400px"/>

#### Q28. L1／L2 正则化与高斯先验／对数先验的 MAP 贝叶斯推断的关系
- L2 正则化相当于权重是高斯先验的 MAP 贝叶斯推断
- L1 正则化相当于权重是对数先验的 MAP 贝叶斯推断，即权重先验是各项同性的拉普拉斯分布

#### Q29. 什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛
一个例子是应用于线性可分问题的逻辑回归。如果权重向量 w 能够实现完美分类， 那么 2w 也会以更高似然实现完美分类。类似随机梯度下降的迭代优化算法将持续增加的大小，理论上永远不会停止。在实践中，数值实现的梯度下降最终会达到导致数值溢出的超大权重，而正则化可以抑制这一问题。

#### Q30. 为什么考虑在模型训练时对输入 (隐藏单元／权重) 添加方差较小的噪声，与正则化的关系
- 神经网络被证明对噪声不是非常健壮 (Tang and Eliasmith, 2010)。改善神经网络健壮性的方法之一是简单地将随机噪声添加到 输入再进行训练。向隐藏单元施加噪声也是可行的，这可以被看作在多个抽象层上进行的数据集增强。
- 最小化带权重噪声的目标函数等同于最小化附加正则化项的目标函数。这种形式的正则化鼓励参数进入权重小扰动对输出相对影响较小的参数空间区域。换句话说，它推动模型进入对权重小的变化相对不敏感的区域，找到的点不只是极小点，还是由平坦区域所包围的极小点。