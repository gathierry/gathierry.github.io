---
layout: post
title: 降维方法 LDA 与 PCA
tags: [data analysis, machine learning]
description: 
date: 2018-08-16
feature_image: 
usemathjax: true
---

LDA 和 PCA 是两种常用的降维方法，在特征数量过多的时候，可以用来压缩特征的维度，从而减少计算量。

<!--more-->

## LDA

LDA 是一种常用的**监督**降维方法。LDA 在降维时注意保留类间距和类内距的信息。  
假设有 $$N$$ 组训练数据，其中第一类 $$n_1$$ 个，第二类 $$n_2$$ 个，...，第 K 类 $$n_K$$ 个，特征数是 $$m$$。

- Within-class scatter matrix $$S_W$$

$$ S_W = \sum_{i=1}^K S_i \\
S_i = (N_i - 1)\Sigma_i = \sum_{x\in Class_i}(x-\mu_i)(x-\mu_i)^\top $$

- Between-class scatter matrix $$S_B$$

$$ S_B = \sum_{i=1}^K N_i (\mu_i-\mu_{global})(\mu_i-\mu_{global})^\top$$

- Global scatter matrix $$S_T$$

$$ S_T=\sum_x (x-\mu)(x-\mu)^\top $$

$$S^T = S^B+S^W $$

定义变换后 $$y=w^\top x$$，$$w$$ 是一个 $$m\times d$$ 的矩阵（假设要降到 d 维）

$$\widetilde{S}_W = w^\top S_W w \\
\widetilde{S}_B = w^\top S_B w$$

LDA 的目标是优化 

$$\max_{w}\frac{tr(w^\top S_B w)}{tr(w^\top S_W w)} \\
= \max_{w} \sum_{i=1}^d \frac{w_i^\top S_B w_i}{w_i^\top S_W w_i}$$  

因为对矩阵优化比较困难，所以对每个元素分别优化  
令 $$||w_i^\top S_W w_i||=1$$，问题转化成

$$
\min_w -w_i^\top S_B w_i \\
s.t. ||w_i^\top S_W w_i||=1
$$

拉格朗日法求解

$$
\begin{align}
L &=  -w_i^\top S_B w_i + \lambda (||w_i^\top S_W w_i||-1) \\
\frac{\partial L}{\partial w_i} &= -2S_B w_i + 2\lambda S_W w_i = 0 \\
S_B w_i &= \lambda S_W w_i \\
S_W^{-1}S_Bw_i &= \lambda w_i
\end{align}
$$

所以 $$S_W^{-1}S_B$$ 的特征向量就是我们要求的 $$w_i$$。根据特征值取最大的 d 个特征值所对应的特征向量。

## PCA

PCA 是一种常用的**非监督**降维方法。  
假设训练数据 $$X$$ 是一个 $$m \times n$$ 的矩阵，$$m$$ 是特征数， $$n$$ 是样本数。我们希望在向量空间中找到 $$k<m$$ 个新的正交基，来描述特征。同时我们希望在新的基底的方向上，样本的方差最大。

首先我们对所有特征归一化使均值为 0。这时可计算协方差矩阵
\\[
\Sigma_X=\frac{1}{n}XX^\top
\\]
对角线上的元素依次是每个特征的方差，其他元素则是两个不同特征的协方差。我们希望得到的一组正交基，即对角线以外的元素都是 0。  
令 $$Y=PX$$，Y的协方差矩阵是对角矩阵

$$\begin{align}
\Sigma_Y&=\frac{1}{n}YY^\top \\
&=\frac{1}{n}PX(PX)^\top \\
&= \frac{1}{n}PXX^\top P^\top \\
&= P \Sigma_X P^\top
\end{align}$$

同时已知$$\Sigma_X$$ 是实对称矩阵，通过求特征值和特征向量可以将矩阵 $$\Sigma_X$$ 对角化。

$$
\Sigma_X V=V\Lambda  \\
\Lambda = V^{-1}\Sigma_X V
$$

因为实对称矩阵的特征向量正交（证明在文末），所以 $$V^{-1}=V^\top$$，上式中的 $$P$$ 其实就是特征向量组成的矩阵。  
PCA 的过程就是求出特征协方差矩阵的特征值和特征向量，选择最大的 k 个特征值（该方向上方差大）所对应的特征向量作为新的基底。

#### 证明对称矩阵 A 的特征向量正交。  

已知任意 $$i \neq j$$

$$
Ax_i = \lambda_ix_i \\
Ax_j = \lambda_jx_j
$$

可得

$$
x_j^\top A x_i = \lambda_i x_j^\top x_i \\
x_j^\top A^\top x_i = \lambda_i x_j^\top x_i \\
(Ax_j)^\top x_i = \lambda_i x_j^\top x_i \\
\lambda_j x_j^\top x_i = \lambda_i x_j^\top x_i
$$

因为 $$\lambda_i \neq \lambda_j$$，所以$$x_j^\top x_i = 0 $$

*参考资料 http://blog.codinglabs.org/articles/pca-tutorial.html*

