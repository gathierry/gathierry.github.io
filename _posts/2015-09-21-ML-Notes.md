---
layout: post
title: Machine Learning Notes
tags: [machine learning]
description: Personal note for machine learning course of Andrew Ng.
date: 2015-09-21
feature_image: images/2015-09-21-ML-Notes/title.png
usemathjax: true
---
The Machine Learning by Andrew Ng is one of the most popular courses on Coursera. It not only introduced basic theories, but also showed advanced applications.

<!--more-->

# Lecture 1

## Introduction of Machine Learning
The machine learning could be divided into 2 parts

### Supervised Learning
- Regression
- Classification (labeled)

### Unsupervised Learning
- Clustering the data

## Univariable linear regression
- Hypothesis function

  $$ h_{\theta}(x)=\theta_0 + \theta_1 x $$

- Cost function  
  The cost function for univariable linear regression is a [convex function](https://en.wikipedia.org/wiki/Convex_function) (or called bowl-shaped function). Therefore, it only has one global minimum point

  \\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \\]

- Gradient descend  
  $$ \alpha $$ is the learning rate that can't be too big or too small. A big rate could lead to a divergence instead of a convergence we expect. And a tiny rate would make the process too slow.
  
    $$ \theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j} $$

  Therefore we can conclude that

  $$ J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(\theta_0 + \theta_1 x^{(i)} -y^{(i)}))^2 $$

  Deriving the function, we got

\\[\begin{aligned}
   \theta_0:=\theta_0-\alpha \frac{1}{m}\sum\_{i=1}^{m}(h\_{\theta}(x^{(i)})-y^{(i)})) \\\
 \theta_1:=\theta_1-\alpha \frac{1}{m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}) 
 \end{aligned} \\]

  m - the size of the training examples  
  x - input  
  y - output  
  This method is called **Batch Gradient Descent** when we use all the training examples



# Lecture 2

## Multiple variables

For a multivatiate linear regression, we take

\\[x =  \begin{vmatrix}
x_0 \\\
x_1 \\\
x_2 \\\
. \\\
. \\\
. \\\
x_n
\end{vmatrix} \quad \text{where } x_0=1 \\]


\\[\theta^T =  \begin{vmatrix}
\theta_0 &  \theta_1 & \theta_2 & ... & \theta_n
\end{vmatrix} \\]

The hypothesis function is 

$$ h_{\theta}(x)=\theta^Tx $$

Given the cost function 

\\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \\]

we could deduce the gradient descent 

\\[
 \theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j} = \theta_j-\alpha \frac{1}{m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}) 
\\]

## Feature scaling

The goal of feature scaling is to make sure the features are on a similar scale. It could accelerate the convergence  
**Mean normalization**

  \\[
  x_j:=\frac{x_j-\mu_j}{(max - min)}\quad \text{or}\quad  x_j:=\frac{x_j-\mu_j}{\sigma}
  \\]

## Tricks for the debugging

- To make sure the gradient descent working correctly, with an increasing number of iteration, the J should decrease

- Choose the learning rate 0.001 -> 0.003 -> 0.01 -> 0.03 -> 0.1 -> 0.3 -> 1 

- Polynomial regression
  $$ h_{\theta}(x)=\theta_0+\theta_1x+\theta_2x^2=\theta_0+\theta_1x_1+\theta_2x_2 $$
  In this case, the feature scale becomes more important.
  However, the square function will finally decrease with a great x. To avoid that, we could choose another model

  $$ h_{\theta}(x)=\theta_0+\theta_1x+\theta_2\sqrt{x} $$



# Lecture 3

## Normal equation

Apart from the gradient descent, normal equation is another method to find the minimum J analytically. From the last part, we know the cost function and the input x for one example of n features 

\\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \\]

\\[x^{(i)} =  \begin{vmatrix}
x^{(i)}_0 \\\
x^{(i)}_1 \\\
x^{(i)}_2 \\\
. \\\
. \\\
. \\\
x^{(i)}_n
\end{vmatrix} \quad \text{where } x_0=1 \\]

Then we got a **design matrix** that is an m by (n+1) dimensional matrix

\\[X =  \begin{vmatrix}
... & ^tx^{(1)} & ... \\\
... & ^tx^{(2)} & ... \\\
... & ^tx^{(3)} & ... \\\
&. \\\
&. \\\
&. \\\
... & ^tx^{(m)} & ... 
\end{vmatrix} \\]

Whereas y is a vector of dimension m

\\[y =  \begin{vmatrix}
y^{(1)} \\\
y^{(2)} \\\
y^{(3)} \\\
. \\\
. \\\
. \\\
y^{(m)}
\end{vmatrix} \\]

To minimize the cost function, we calculate the $$\theta$$ from

\\[
\boxed{\theta=(X^TX)^{-1}X^Ty}
\\]

### Characteristics

- Advantages
  - no need to do the feature scaling
  - no need to choose the learning rate
  - no need to iterate

- Disadvantage
  - The calculation of the $$ (X^TX)^{-1} $$ could be very slow if n is large (>10000) since the complexity for a matrix n by n is $$ O(n^3) $$

If $$ (X^TX) $$ is non-invertible (singular / degenerate), it could be caused by 2 reasons:

- Redundant feature, which leads to linear dependences
- Too many features ($$m \leq n$$)

In the second case, we can delete some features or use the **regularization** that we'll talk about later.



# Lecture 4

## Classification

For the classifcation problem, the output y is no more continue. Instead, $$ y\in \{0, 1\} $$ for a **binary class classification** or $$ y\in \{1, 2, 3, ... , p\} $$ for a **multiclass classification**.

### Hypothesis function

\\[
h\_\theta (x)=g(\theta ^T x)=\frac{1}{1+e^{-\theta ^T x}} \quad 0\leq h_\theta (x) \leq 1
\\]  

This is a **sigmoid (or logistic) function** of $$ \theta ^T x $$   
<img src="/images/2015-09-21-ML-Notes/logisticfunction.jpg" width="400px"/>

#### Hypothesis representation

$$ h_\theta (x)=P(y=1|x; \theta) $$ represent the probability that $$y=1$$ given $$x$$ parameterized by $$\theta$$. So $$ P(y=0|x; \theta)=1-h_\theta (x) $$

#### Decision boundary

Observing the graph of the sigmoid function, it's clear that 
\\[
y = 1 \quad if\ h\_\theta (x) \geq 0.5 \Leftrightarrow \theta ^Tx \geq 0 \\\
y = 0 \quad if\ h_\theta (x) < 0.5 \Leftrightarrow \theta ^Tx < 0
\\]

For example, $$h_\theta = g(-3 + x_1 + x_2)$$, y=1 if $$-3 + x_1 + x_2 \geq 0$$
<img src="/images/2015-09-21-ML-Notes/decisionboundary.jpg" width="400px"/>  
The straight line is the **decision boundary** which is a property of the hypothesis and the parameters

A non-linear example: $$ h_\theta (x)=g(-1+x_1^2+x_2^2) $$  
<img src="/images/2015-09-21-ML-Notes/circledecisionboundary.jpg" width="400px"/>  

### Cost function

\\[
J(\theta)=\frac{1}{m}\sum\_{i=1}^m \underbrace{(-y^{(i)}log(h\_\theta(x^{(i)}))-(1-y^{(i)})log(1-h\_\theta(x^{(i)})))}\_{cost(h_\theta(x),\ y)}
\\]

### Gradient descent

\\[
\theta_j = \theta_j - \alpha \frac{1}{m} \sum\_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
\\]

### Optimization algorithms

To speed up the computation, we can use the optimization algorithms in Matlab.  
First, implement the cost function: ```func[jVal, gradient]=costFunction(theta)```. jVal is the cost function $$J(\theta)$$ and gradient is a matrix 
$$
\begin{vmatrix}
\frac{\partial J}{\partial \theta_0} \\\
\frac{\partial J}{\partial \theta_1} \\\
. \\\
. \\\
. \\\
\frac{\partial J}{\partial \theta_n}
\end{vmatrix}
$$
Second, call the function ```fminunc```. In this function $$dim(\theta) \geq 2$$

	options = optimset('GradObj', 'on', 'MaxIter', '100');
	initialTheta = zeros(n, 1);
	[optTheta, cost] = fminunc(@(t)(costFunction(t, X, y)), initialTheta, options);
	% optTheta is the optimized theta we need
	% cost should be near to 0
	% t the argement that calls the cost function, no need to declare before

## Multiclass classification

The algorithm we use here is called **one-vs-all**  
<img src="/images/2015-09-21-ML-Notes/one-vs-all.png" width="400px"/>  
For an input x, pick the class j that maxmizes $$h_\theta^{(j)}(x)$$



# Lecture 5

- underfitting = high bias
- overfitting = high variance

To avoid the overfitting, we could use the **regularization** method.

## Linear regression

### Gradient descent

Cost function

\\[
J(\theta_0, \theta_1)=\frac{1}{2m}\sum\_{i=1}^{m}(h\_{\theta}(x^{(i)})-y^{(i)}))^2+\lambda \sum^n_{j=1}\theta_j^2
\\]

In the term added, notice that **j is from 1** instead of 0. $$\lambda$$ is the **regularization parameter**. A too large $$\lambda$$ will cause the underfitting since all $$\theta_j \simeq 0$$.

Gradient descent

\\[
 \theta_0:=\theta_0-\alpha\frac{\partial J}{\partial \theta_j} = \theta_0-\alpha \frac{1}{m}\sum\_{i=1}^{m}(h\_{\theta}(x^{(i)})-y^{(i)})x_0^{(i)}) \\\
 \theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j} = \theta_j-\alpha [\frac{1}{m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
\\]

### Normal equation

\\[
\theta=(X^TX+\begin{vmatrix}
0 & 0 & 0 & ... & 0 \\\
0 & \lambda & 0 & ... & 0 \\\
0 & 0 & \lambda & ... & 0  \\\
 & & & ... \\\
0 & 0 & 0 & ... & \lambda
\end{vmatrix})^{-1}X^Ty
\\]

The matrix added is an square matrix of dimension (n+1)

## Logistic regression

Cost function

\\[
J(\theta)=-\frac{1}{m}\sum\_{i=1}^m (y^{(i)}log(h\_\theta(x^{(i)}))+(1-y^{(i)})log(1-h\_\theta(x^{(i)}))) + \frac{\lambda}{2m}\sum\_{j=1}^n \theta_j^2
\\]

Gradient descent

\\[
\theta_0 = \theta_0 - \alpha \frac{1}{m} \sum\_{i=1}^m (h\_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\\
\theta_j = \theta_j - \alpha [\frac{1}{m} \sum\_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j]
\\]



# Lecture 6

To solve a classification problem with neural network, we usually do these following steps:

## 1. Pick a network architecture

- Number of input units = dimension of features
- Number of output units = number of classes
- the number of hidden layer and hidden units depends

## 2. Randomly initialize weights

Considering the bias unit, we add layer_in with 1. And we declare $$\epsilon$$ so that all elements in W are in $$[-\epsilon, \epsilon]$$

	W = rand(layer_out, 1 + layer_in) * 2 * epsilon - epsilon;

## 3. Forward propagation to compute $$a^{(L)}$$

In a neural network model, 
\\[
a^{(L)} = g((\theta^{(L-1)})^Ta^{(L-1)})
\\]
where g is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) and $$a^{(L-1)}$$ contains a bias unit.

## 4. Compute the cost function J($$\theta$$)

The cost function of the neural network is :

$$
J(\theta)=-\frac{1}{m}\sum^m_{i=1}\sum^K_{k=1}[y_k^{(i)}log(h_{\theta}(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_{\theta}(x^{(i)}))_k)] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^{(l)})^2
$$

- m is the number of training set
- K is the number of units in the output layer
- $$s_l$$ is the number of unit in layer $$l$$
- L is the total number of layers

When $$\lambda \neq 0$$, the function is regularized. We take no account of the bias units for the regularization.

	hidden = sigmoid([ones(m, 1) X] * Theta1');
	h_theta = sigmoid([ones(m, 1) hidden] * Theta2');
	term1 = (-y_code)' * log(h_theta);
	term2 = (1-y_code)' * log(1-h_theta);
	J = 1/m * (sum(term1(:)) - sum(term2(:)));
	
	reg_theta1 = Theta1(:, 2:end);
	reg_theta2 = Theta2(:, 2:end);
	reg_term = lambda / (2*m) * (reg_theta1(:)' * reg_theta1(:)...
	                       + reg_theta2(:)' * reg_theta2(:));
	J = J + reg_term;

## 5. Back propagation to compute $$\frac{\partial J(\theta)}{\partial \theta_{ij}^{(l)}} $$

### for i = 1 to m

- First, get the values for all units

  	% code for m training set
  	a1 = [ones(m, 1), X];
  	z2 = a1 * Theta1';
  	a2 = [ones(m, 1), sigmoid(z2)];
  	z3 = a2 * Theta2';
  	a3 = sigmoid(z3);

- Second, define $$\delta_j^{(l)}$$ error of node j in layer l.  
  For the output unit, $$\delta^{(L)}=a^{(L)}-y$$  
  For l = (L-1), ..., 2, $$\delta^{(l)}=((\theta^{(l)})^T)\delta^{(l+1)}.*\underbrace{g'(z^{(l)})}_{a^{(l)}.*(1-a^{(l)})}$$ 

  	% code for m training set
  	delta3 = a3 - y_code;
  	delta2 = delta3 * Theta2(:, 2:end) .* sigmoidGradient(z2);

- Third, suppose $$\Delta^{(l)} = \Delta^{(l)}+\delta^{(l+1)}a^{(l)T}$$
  	

  	% code for m training set
  	Delta1 = delta2' * a1;
  	Delta2 = delta3' * a2;

- Finally,   

$$\frac{\partial J(\theta)}{\partial \theta_{ij}^{(l)}}=D_{ij}^{(l)}=\frac{1}{m}\Delta^{(l)}_{ij}+\lambda \theta_{ij}^{(l)} \quad j\neq0$$

$$\frac{\partial J(\theta)}{\partial \theta_{ij}^{(l)}}=D_{ij}^{(l)}=\frac{1}{m}\Delta^{(l)}_{ij} \quad j=0$$

## 6. Gradient checking

To make sure that the back propagation didn't go wrong, the method gradient checking is used.
Suppose e=[0 0 ... $$\epsilon$$ ... 0 0] that $$\epsilon$$ is the $$l^{th}$$ element.  
If 
$$
D^{(l)}\approx\frac{1}{2\epsilon}(J(\theta+e)-J(\theta-e))
$$, the propagation will work.


## 7. Minimize J

Since we have both the cost function and its partial derivatives, we can minimize J with gradient descent or other advanced optimization.



# Lecture 7

Apart from the logistic regression, Support Vector Machine is another powerful algorithm for classification problems.

##  Gaussian kernel

Knowing a training data set $$ x^{(1)},\ x^{(2)} ...,\ x^{(m)} $$, we put all these training examples to landmarks $$ l^{(i)}= x^{(i)}$$. The kernel function

\\[f^{(i)} =  \begin{vmatrix}
f^{(i)}_0 \\\
f^{(i)}_1 \\\
f^{(i)}_2 \\\
. \\\
. \\\
. \\\
f^{(i)}_m
\end{vmatrix} \quad \text{where } f_0=1 \\]

\\[
f^{(i)}_j=exp(-\frac{\|x-l^{(i)}\|^2}{2\sigma ^2})
\\]

Gaussian kernel is used when we have a large data set and a intermediate number of features.

Code Matlab to implement a gaussian kernel function

	function sim = gaussianKernel(x1, x2, sigma)
	    % Ensure that x1 and x2 are column vectors
	    x1 = x1(:); x2 = x2(:);
	    sim = exp(- (x1 - x2)' * (x1 - x2) / 2 / (sigma^2));   
	end

## Linear kernel

\\[f^{(i)} = x^{(i)}
\\]

Linear kernel is used when we have a large data set and a small number of features or a small data set and a large number of features. Logistic regression could work in such situations as well.

## SVM Training

__First, do perform feature scaling__

Second, train with the cost function :
\\[J = C\sum\_{i=1}^{m}y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})+\frac{1}{2}\sum_{j=1}^{m}\theta_j^2
\\]

cost1 is a function like this

<img src="/images/2015-09-21-ML-Notes/svm-cost1.jpg" width="400px"/> 

while cost0 is 

<img src="/images/2015-09-21-ML-Notes/svm-cost0.jpg" width="400px"/>

- Choice of C (= $$\frac{1}{\lambda}$$)
  - a large C leads to a low bias and a high variance (overfitting) 
  - a small C leads to a high bias and a low variance (underfitting)

- Choice of $$\sigma^2$$
  - a large $$\sigma^2$$ leads to a high bias and a low variance (underfitting)  
  - a small $$\sigma^2$$ leads to a low bias and a high variance (overfitting)

Finally, to predict, we got
\\[
y = 1 \quad if\ \theta^Tf\geq0
\\]



# Lecture 8

## K-Means algorithm

Inputs : 

- K (number of clusters)
- Training set $${x^{(1)}, ... , x^{(m)}}$$

Cost function :

\\[J = \frac{1}{m}\sum\_{i=1}^{m}\|x^{(i)}-\mu_{c^{(i)}}\|^2
\\]

We could repeat the following process for 50 - 1000 times and pick the clustering that gives the lowest cost.

1. Ramdomly initialize K cluster centroids $$\mu_1, ... ,\mu_k $$  
   Randomly pick K training examples  
   Set $$\mu_1, ... ,\mu_k $$ equal to these examples

2. Repeat  
   - Cluster assignment step  
     for i=1:m  
      $$c_i$$ = index of cluster centroid (1~K) closest to $$x_i$$
   - Move centroid  
     for k=1:K  
     $$\mu_k$$ = mean of points assigned to cluster k



# Lecture 9

**Dimensionality reduction** is used to economise the memory / disk space, speed up learning algorithms and visualize high-dimensional data. However, it is not used to avoid overfitting.


## Principal component analysis

PCA reduces the data from n-D to k-D : find k vectors $$ u^{(1)}, ... , u^{(k)} $$ onto which to project the data so as to minimize the projection error.

### Algorithm

- Knowing the training set $$ x^{(1)}, ... , x^{(m)} \in \mathbb{R}^n $$

- Preprocessing (feature scaling / mean normalization)

\\[
\mu_j = \frac{1}{m}\sum_{i=1}^{m}x_j^{(i)} \\\
\text{Replace } x_j \text{ with } x_j - \mu_j
\\]

if different feature on different scale

\\[
\text{Replace } x_j \text{ with } \frac{x_j - \mu_j}{s_j}
\\]

- Reduce data from n-D to k-D

  - Compute "covariance matrix" $$\Sigma = \frac{1}{m}\sum_{i=1}^m x^{(i)} (x^{(i)})^T$$

  - Compute "eigenvector" of $$\Sigma$$

        [U, S, V] = svd(Sigma);

    \\[
    U = \begin{vmatrix}
    | & | & ... & |\\\
    u^{(1)} & u^{(2)} & ... & u^{(n)} \\\
    | & | & ... & |
    \end{vmatrix}
     \\]
     Pick the first $$ k $$ column to form a new matrix $$U_{reduce}$$  
     New training set $$ z = U_{reduce}^T x $$  
     $$S$$ is a diagonal matrix with eigenvalues of $$ \Sigma $$ 

  - To choose a proper $$ k $$, we need	$$\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99 $$, 
    that means 99% of variance is retained

- Reconstruction from compressed representation : 
  $$ x_{approx}^{(i)} = U_{reduce} z^{(i)} $$  



# Lecture 10

To solve an anomaly detection problem with a training set $$\{x^{(1)}, ..., x^{(m)}\in \mathbb{R}^n \}$$, we assume that

\\[
x_1 \sim N(\mu_1, \sigma_1^2) \\\
... \\\
x_n \sim N(\mu_n, \sigma_n^2)
\\]

in which

\\[
\mu_j = \frac{1}{m}\sum\_{i=1}^{m}x_j^{(i)} \\\
\sigma_j^2 = \frac{1}{m}\sum_{i=1}^{m}(x_j^{(i)}-\mu_j)^2
\\]

To predict a new data x, compute 

\\[p(x) = \prod_{j=1}^n \frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})
\\]

if $$ p(x) < \epsilon $$, the data is anomaly.

