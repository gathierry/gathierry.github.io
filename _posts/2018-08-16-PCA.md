---
layout: post
title: 用 PCA 降维
categories: [data science]
tags: [data analysis, machine learning]
description: PCA 是一种常用的**非监督**降维方法。
---

PCA 是一种常用的**非监督**降维方法。  
假设训练数据 $$X$$ 是一个 $$m \times n$$ 的矩阵，$$m$$ 是特征数， $$n$$ 是样本数。我们希望在向量空间中找到 $$k<m$$ 个新的正交基，来描述特征。同时我们希望在新的基底的方向上，样本的方差最大。

首先我们对所有特征归一化使均值为 0。这时可计算协方差矩阵
\\[
\Sigma_X=\frac{1}{n}XX^\top
\\]
对角线上的元素依次是每个特征的方差，其他元素则是两个不同特征的协方差。我们希望得到的一组正交基，即对角线以外的元素都是 0。  
令 $$Y=PX$$，Y的协方差矩阵是对角矩阵

$$\begin{align}
\Sigma_Y&=\frac{1}{n}YY^\top \\
&=\frac{1}{n}PX(PX)^\top \\
&= \frac{1}{n}PXX^\top P^\top \\
&= P \Sigma_X P^\top
\end{align}$$

同时已知$$\Sigma_X$$ 是实对称矩阵，通过求特征值和特征向量可以将矩阵 $$\Sigma_X$$ 对角化。

$$
\Sigma_X V=V\Lambda  \\
\Lambda = V^{-1}\Sigma_X V
$$

因为实对称矩阵的特征向量正交（证明在文末），所以 $$V^{-1}=V^\top$$，上式中的 $$P$$ 其实就是特征向量组成的矩阵。  
PCA 的过程就是求出特征协方差矩阵的特征值和特征向量，选择最大的 k 个特征值（该方向上方差大）所对应的特征向量作为新的基底。

#### 证明对称矩阵 A 的特征向量正交。  
已知任意 $$i \neq j$$

$$
Ax_i = \lambda_ix_i \\
Ax_j = \lambda_jx_j
$$

可得

$$
x_j^\top A x_i = \lambda_i x_j^\top x_i \\
x_j^\top A^\top x_i = \lambda_i x_j^\top x_i \\
(Ax_j)^\top x_i = \lambda_i x_j^\top x_i \\
\lambda_j x_j^\top x_i = \lambda_i x_j^\top x_i
$$

因为 $$\lambda_i \neq \lambda_j$$，所以$$x_j^\top x_i = 0 $$

*参考资料 http://blog.codinglabs.org/articles/pca-tutorial.html*