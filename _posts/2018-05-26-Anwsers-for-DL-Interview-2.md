---
layout: post
title: 深度学习面试你必须知道这些答案（二）
categories: [data science]
tags: [deep learning]
description: 本文节选了网上分享的一部分深度学习相关的面试题，并将搜集到的答案随之附上。
---
本文节选了网上分享的一部分深度学习相关的面试题，并将搜集到的答案随之附上。  
参考了来自 _https://cloud.tencent.com/developer/article/1064505_ 的内容。

#### Q11. 模型容量，表示容量，有效容量，最优容量概念
- 通过调整模型的容量 (capacity)，我们可以控制模型是否偏向于过拟合或者欠拟合。通俗地，模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟合训练集。容量高的模型可能会过拟合。
- 学习算法可以从哪些函数族中选择函数。这被称为模型的表示容量 (representational capacity)。
- 额外的限制因素，比如优化算法的不完美，意味着学习算法的有效容量 (effective capacity)可能小于模型族的表示容量。

#### Q12. 正则化中的权重衰减与加入先验知识在某些条件下的等价性
- 我们可以加入权重衰减 (weight decay) 来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和，其偏好于平方 L2 范数较小的权重。
- 在进行最大后验估计 (MAP) 时，
\\[
\theta_{MAP}=argmax_\theta \log{p(x|\theta)}+\log{p(\theta)}
\\]
如果先验分布是 $$\mathcal{N}(w; 0, \frac{1}{\lambda}I^2)$$，那么对数先验项正比于熟悉的权重衰减惩罚 $$\lambda w^T w$$。

#### Q13. 高斯分布的广泛应用的缘由
- 我们想要建模的很多分布的真实情况是比较接近正态分布的。 中心极限定理说明很多独立随机变量的和近似服从正态分布。这意味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可以被分解成一些更结构化的部分。
- 在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。正态分布拥有最大的熵，我们通过这个假定来保证了最小可能量的结构。

#### Q14. 最大似然估计中最小化 KL 散度与最小化分布之间的交叉熵的关系
- 一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布和模型分布之间的差异，两者之间的差异程度可以通过 KL 散度度量。
- 最小化 KL 散度其实就是在最小化分布之间的交叉熵。
- 虽然最优 $$\theta$$ 在最大化似然或是最小化 KL 散度时是相同的，但目标函数值是不一样的。在软件中，我们通常将两者都称为最小化代价函数。因此最大化似然变成了最小化负对数似然(NLL)，或者等价的是最小化交叉熵。

#### Q15. 在线性回归问题，具有高斯先验权重的 MAP 贝叶斯推断与权重衰减的关系，与正则化的关系
同 Q13

#### Q16. 稀疏表示，低维表示，独立表示
- 低维表示尝试将 信息尽可能压缩在一个较小的表示中，比如 PCA。
- 稀疏表示将数据集嵌入到输入项大多数为零的表示中，L1 惩罚可以诱导稀疏的参数。稀疏表示通常用于需要增加表示维数的情况，使得大部分为零的表示不会丢失很多信息。这会使得表示的整体结构倾向于将数据分布在表示空间的坐标轴上。
- 独立表示试图分开数据分布中变化的来源，使得表示的维度是统计独立的。

#### Q17. 列举一些无法基于梯度的优化来最小化的代价函数及其具有的特点
维数灾难

#### Q18. 在深度神经网络中，引入了隐藏层，放弃了训练问题的凸性，其意义何在
一些隐藏单元可能并不是在所有的输入点上都是可微的，在实践中，梯度下降对这些机器学习模型仍然表现得足够好。部分原因是神经网络训练算法通常不会达到代价函数的局部最小值，而是仅仅显著地减小它的值。

#### Q19. 函数在某个区间的饱和与平滑性对基于梯度的学习的影响
最广泛使用的隐式“先验”是平滑先验，或局部不变性先验。这个先验表明我们学习的函数不应在小区域内发生很大的变化。许多简单算法完全依赖于此先验达到良好的泛化，其结果是不能推广去解决人工智能级别任务中的统计挑战。

#### Q20. 梯度爆炸的一些解决办法
参考 _https://blog.csdn.net/qq_25737169/article/details/78847691_

- 梯度爆炸会导致结果不收敛。
- 梯度爆炸问题可以通过梯度截断来缓解(执行梯度下降步骤之前设置梯度的阈值)。
- 较大的权重也会产生使得激活函数饱和的值，导致饱和单元的梯度完全丢失。通过正则化和 Batch Normalization可以解决。
- 激活函数如 Relu 使导数变为 1，也可以解决梯度爆炸的问题。

_对于梯度消失，除了以上几种方案，Residual block 的 shortcut 可以用来解决_
