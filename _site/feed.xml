<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Stylish Jekyll Theme</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>NumPy Tutorial (2)</title>
				<description>
</description>
				<pubDate>Tue, 28 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/28/NumPy-Basic-2.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/28/NumPy-Basic-2.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (5)</title>
				<description>&lt;ul&gt;
  &lt;li&gt;underfitting = high bias&lt;/li&gt;
  &lt;li&gt;overfitting = high variance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To avoid the overfitting, we could use the &lt;strong&gt;regularization&lt;/strong&gt; method.&lt;/p&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear regression&lt;/h2&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient descent&lt;/h3&gt;
&lt;p&gt;Cost function&lt;/p&gt;

&lt;p&gt;\[
J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2+\lambda \sum^n_{j=1}\theta_j^2
\]&lt;/p&gt;

&lt;p&gt;In the term added, notice that &lt;strong&gt;j is from 1&lt;/strong&gt; instead of 0. &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is the &lt;strong&gt;regularization parameter&lt;/strong&gt;. A too large &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; will cause the underfitting since all &lt;script type=&quot;math/tex&quot;&gt;\theta_j \simeq 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Gradient descent&lt;/p&gt;

&lt;p&gt;\[
 \theta_0:=\theta_0-\alpha\frac{\partial J}{\partial \theta_j} = \theta_0-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_0^{(i)}) \\
 \theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j} = \theta_j-\alpha [\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
\]&lt;/p&gt;

&lt;h3 id=&quot;normal-equation&quot;&gt;Normal equation&lt;/h3&gt;

&lt;p&gt;\[
\theta=(X^TX+\begin{vmatrix}
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; … &amp;amp; 0 \\
0 &amp;amp; \lambda &amp;amp; 0 &amp;amp; … &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; \lambda &amp;amp; … &amp;amp; 0  \\
 &amp;amp; &amp;amp; &amp;amp; … \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; … &amp;amp; \lambda
\end{vmatrix})^{-1}X^Ty
\]&lt;/p&gt;

&lt;p&gt;The matrix added is an square matrix of dimension (n+1)&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic regression&lt;/h2&gt;
&lt;p&gt;Cost function&lt;/p&gt;

&lt;p&gt;\[
J(\theta)=\frac{1}{m}\sum_{i=1}^m (-y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))) + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2
\]&lt;/p&gt;

&lt;p&gt;Gradient descent&lt;/p&gt;

&lt;p&gt;\[
\theta_0 = \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\
\theta_j = \theta_j - \alpha [\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j]
\]&lt;/p&gt;
</description>
				<pubDate>Mon, 27 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/27/ML-Notes-5.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/27/ML-Notes-5.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (4)</title>
				<description>&lt;h2 id=&quot;classification&quot;&gt;Classification&lt;/h2&gt;
&lt;p&gt;For the classifcation problem, the output y is no more continue. Instead, &lt;script type=&quot;math/tex&quot;&gt;y\in \{0, 1\}&lt;/script&gt; for a &lt;strong&gt;binary class classification&lt;/strong&gt; or &lt;script type=&quot;math/tex&quot;&gt;y\in \{1, 2, 3, ... , p\}&lt;/script&gt; for a &lt;strong&gt;multiclass classification&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;hypothesis-function&quot;&gt;Hypothesis function&lt;/h3&gt;
&lt;p&gt;\[
h_\theta (x)=g(\theta ^T x)=\frac{1}{1+e^{-\theta ^T x}} \quad 0\leq h_\theta (x) \leq 1
\]&lt;/p&gt;

&lt;p&gt;This is a &lt;strong&gt;sigmoid (or logistic) function&lt;/strong&gt; of &lt;script type=&quot;math/tex&quot;&gt;\theta ^T x&lt;/script&gt; &lt;br /&gt;
&lt;img src=&quot;/images/logisticfunction.jpg&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;hypothesis-representation&quot;&gt;Hypothesis representation&lt;/h4&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;h_\theta (x)=P(y=1|x; \theta)&lt;/script&gt; represent the probability that &lt;script type=&quot;math/tex&quot;&gt;y=1&lt;/script&gt; given &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; parameterized by &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. So &lt;script type=&quot;math/tex&quot;&gt;P(y=0|x; \theta)=1-h_\theta (x)&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;decision-boundary&quot;&gt;Decision boundary&lt;/h4&gt;
&lt;p&gt;Observing the graph of the sigmoid function, it’s clear that 
\[
y = 1 \quad if\ h_\theta (x) \geq 0.5 \Leftrightarrow \theta ^Tx \geq 0 \\
y = 0 \quad if\ h_\theta (x) &amp;lt; 0.5 \Leftrightarrow \theta ^Tx &amp;lt; 0
\]&lt;/p&gt;

&lt;p&gt;For example, &lt;script type=&quot;math/tex&quot;&gt;h_\theta = g(-3 + x_1 + x_2)&lt;/script&gt;, y=1 if &lt;script type=&quot;math/tex&quot;&gt;-3 + x_1 + x_2 \geq 0&lt;/script&gt;
&lt;img src=&quot;/images/decisionboundary.jpg&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
The straight line is the &lt;strong&gt;decision boundary&lt;/strong&gt; which is a property of the hypothesis and the parameters&lt;/p&gt;

&lt;p&gt;A non-linear example: &lt;script type=&quot;math/tex&quot;&gt;h_\theta (x)=g(-1+x_1^2+x_2^2)&lt;/script&gt;&lt;br /&gt;
&lt;img src=&quot;/images/circledecisionboundary.jpg&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cost-function&quot;&gt;Cost function&lt;/h3&gt;
&lt;p&gt;\[
J(\theta)=\frac{1}{m}\sum_{i=1}^m \underbrace{(-y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))}_{cost(h_\theta(x),\ y)}
\]&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient descent&lt;/h3&gt;
&lt;p&gt;\[
\theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
\]&lt;/p&gt;

&lt;h3 id=&quot;optimization-algorithms&quot;&gt;Optimization algorithms&lt;/h3&gt;
&lt;p&gt;To speed up the computation, we can use the optimization algorithms in Matlab.&lt;br /&gt;
First, implement the cost function: &lt;code&gt;func[jVal, gradient]=costFunction(theta)&lt;/code&gt;. jVal is the cost function &lt;script type=&quot;math/tex&quot;&gt;J(\theta)&lt;/script&gt; and gradient is a matrix 
&lt;script type=&quot;math/tex&quot;&gt;\begin{vmatrix}
\frac{\partial J}{\partial \theta_0} \\\
\frac{\partial J}{\partial \theta_1} \\\
. \\\
. \\\
. \\\
\frac{\partial J}{\partial \theta_n}
\end{vmatrix}&lt;/script&gt;&lt;br /&gt;
Second, call the function &lt;code&gt;fminunc&lt;/code&gt;. In this function &lt;script type=&quot;math/tex&quot;&gt;dim(\theta) \geq 2&lt;/script&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, &#39;100&#39;);
initialTheta = zeros(n, 1);
[optTheta, cost] = fminunc(@(t)(costFunction(t, X, y)), initialTheta, options);
% optTheta is the optimized theta we need
% cost should be near to 0
% t the argement that calls the cost function, no need to declare before
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;multiclass-classification&quot;&gt;Multiclass classification&lt;/h2&gt;
&lt;p&gt;The algorithm we use here is called &lt;strong&gt;one-vs-all&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;/images/one-vs-all.png&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
For an input x, pick the class j that maxmizes &lt;script type=&quot;math/tex&quot;&gt;h_\theta^{(j)}(x)&lt;/script&gt;&lt;/p&gt;
</description>
				<pubDate>Sun, 26 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/26/ML-Notes-4.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/26/ML-Notes-4.html</guid>
			</item>
		
			<item>
				<title>NumPy Tutorial (1)</title>
				<description>&lt;h2 id=&quot;multidimensional-data-structure---ndarray&quot;&gt;Multidimensional data structure - ndarray&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;In [3]: import numpy as np
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;initiation&quot;&gt;Initiation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;vector column&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [4]: vector = [1, 2, 3]
  In [5]: arr_vec = np.array(vector)
  In [6]: arr_vec
  Out[6]: array([1, 2, 3])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;matrix&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [10]: matrix = [[1, 2, 3], [4, 5, 6]]
  In [11]: arr_mat = np.array(matrix)
  In [12]: arr_mat
  Out[12]: 
  array([[1, 2, 3],
         [4, 5, 6]])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;others&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [13]: np.zeros(3)
  Out[13]: array([ 0.,  0.,  0.])
  In [14]: np.ones((2, 3))
  Out[14]: 
  array([[ 1.,  1.,  1.],
         [ 1.,  1.,  1.]])
  In [15]: np.eye(2)
  Out[15]: 
  array([[ 1.,  0.],
         [ 0.,  1.]])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scalar-operations------&quot;&gt;Scalar operations (+ - * / **)&lt;/h3&gt;
&lt;p&gt;Instead of the multiplication between matrix, * is the multiplication between 2 elements of the same index&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [16]: arr_mat
Out[16]: 
array([[1, 2, 3],
       [4, 5, 6]])
In [17]: arr_mat * arr_mat
Out[17]: 
array([[ 1,  4,  9],
       [16, 25, 36]])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;indexing-and-slicing&quot;&gt;Indexing and slicing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;vector&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [19]: arr = np.arange(10)
  In [20]: arr
  Out[20]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
  In [21]: arr[3]
  Out[21]: 3
  In [22]: arr[0:3]
  # 0:3 means 0, 1, 2
  Out[22]: array([0, 1, 2])
  In [23]: arr[0:3] = 10
  In [24]: arr
  Out[24]: array([10, 10, 10,  3,  4,  5,  6,  7,  8,  9])
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;A slice is part of the array. If the value of a slice is changed, so will be the value in the original array&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [28]: slice = arr[5:7]
  In [29]: slice[1] = 0
  In [30]: arr
  Out[30]: array([10, 10, 10,  3,  4,  5,  0,  7,  8,  9])
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;But it could be copied if followed by &lt;code&gt;arr[5:7].copy()&lt;/code&gt;. Howerver, the code below won’t change the array either. We’ll talk about this later in the &lt;em&gt;Fancy indexing&lt;/em&gt; section.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [36]: slice = arr[0]
  In [37]: slice = 99
  In [38]: arr
  Out[38]: array([10, 10, 10,  3,  4,  5,  0,  7,  8,  9])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;matrix&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [44]: matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
  In [45]: arr2d = np.array(matrix)
  In [46]: arr2d[2]
  Out[46]: array([7, 8, 9])
  In [47]: arr2d[0][2]
  Out[47]: 3
  In [56]: arr2d[:2, 1:]
  # [0:2] and [1:max+1]
  Out[56]: 
  array([[2, 3],
         [5, 6]])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fancy-indexing&quot;&gt;Fancy indexing&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;In [61]: new_arr = arr2d[[0, 2]]
In [62]: new_arr
Out[62]: 
array([[1, 2, 3],
       [7, 8, 9]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fancy indexing is different from the slicing. The fancy indexing is a copy of the value while the slicing is a copy of the reference.&lt;/p&gt;

&lt;h3 id=&quot;transpose&quot;&gt;Transpose&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;In [63]: new_arr.T
Out[63]: 
array([[1, 7],
       [2, 8],
       [3, 9]])
&lt;/code&gt;&lt;/pre&gt;

</description>
				<pubDate>Thu, 23 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/23/NumPy-Basic-1.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/23/NumPy-Basic-1.html</guid>
			</item>
		
			<item>
				<title>IntelliJ IDEA CE + Tomcat + Openshift 开发记录</title>
				<description>&lt;h3 id=&quot;section&quot;&gt;开发环境：&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;IntelliJ IDEA CE 14&lt;/li&gt;
  &lt;li&gt;Apache Tomcat 8.0.18&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;项目目录&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Spage(Projet)
├─src
│ └─java
├─lib
├─doc
└─webRoot 
  ├─WEB-INF
  │ ├─classes
  │ ├─lib
  │ └─web.xml
  ├─dist （jar、war的存放路径）
  ├─css
  ├─js
  ├─view
  └─image
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-2&quot;&gt;开发流程&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;设置Java class文件输出目录。右键Module，Open module Settings，paths，将output path设为WEB-INF下的classes文件夹 &lt;strong&gt;注意，实在Module的Path选项卡里修改output path&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;将jsp-api-2.1.jar, servlet-api-2.5.jar拷贝到webRoot下。打包时需要的jar包放到lib目录。&lt;/li&gt;
  &lt;li&gt;右键Module，Open module Settings，Dependencies，+号引入刚刚拷贝进来的jar文件（和lib目录）&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;src目录建test Package，下建Test.java&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  public class Test extends HttpServlet {
      protected void doGet(HttpServletRequest req, HttpServletResponse res) throws ServletException, IOException{
          System.out.println(&quot;hellow world!&quot;);
          res.getWriter().println(&quot;Hellow world!&quot;);
      }
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在web.xml中注册该servlet&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  &amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;?&amp;gt;
  &amp;lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot;
   xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
   xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee
                http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot;
   version=&quot;3.1&quot;&amp;gt;
   &amp;lt;servlet&amp;gt;
   		&amp;lt;servlet-name&amp;gt;test&amp;lt;/servlet-name&amp;gt;
   		&amp;lt;servlet-class&amp;gt;test.Test&amp;lt;/servlet-class&amp;gt;
   	&amp;lt;/servlet&amp;gt;
   	&amp;lt;servlet-mapping&amp;gt;
   		&amp;lt;servlet-name&amp;gt;test&amp;lt;/servlet-name&amp;gt;
   		&amp;lt;url-pattern&amp;gt;/test&amp;lt;/url-pattern&amp;gt;
   	&amp;lt;/servlet-mapping&amp;gt;
   	&amp;lt;session-config&amp;gt;
   		&amp;lt;session-timeout&amp;gt;30&amp;lt;/session-timeout&amp;gt;
   	&amp;lt;/session-config&amp;gt;
  &amp;lt;/web-app&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;右键module，compile module&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在tomcat下 conf\catalina\localhost 创建xml文件（需要sudo）&lt;br /&gt;
&lt;strong&gt;注意，如果path=”/hello”，则文件并也要为hello.xml&lt;/strong&gt;&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  &amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;
  &amp;lt;Context path=&quot;/hello&quot; docBase=&quot;/Users/Thierry/Desktop/Spage/Spage/webRoot&quot; debug=&quot;0&quot; privileged=&quot;true&quot;&amp;gt;
  &amp;lt;/Context&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;webRoot下建index.html&lt;/li&gt;
  &lt;li&gt;浏览器localhost:8080/hello中可以看到index.html的内容&lt;br /&gt;
浏览器localhost:8080/hello/test中可以触发test.java的内容&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;openshift&quot;&gt;Openshift部署&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;添加应用&lt;/li&gt;
  &lt;li&gt;checkout到本地&lt;/li&gt;
  &lt;li&gt;配置pom.xml文件&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;同步代码
以下以Spage项目为例&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; projet-logiciel/
 ├─src/
 │  └─main/
 │     ├─java/
 │     │  ├─db/
 │     │  ├─spider/
 │     │  └─web/
 │     │     ├─SysContextListener.java
 │     │     └─Result.java
 │     ├─resources/
 │     └─webapp/
 │         ├─css/
 │         ├─img/
 │         ├─js/
 │         ├─WEB-INF/
 │         │  ├─lib/
 │         │  ├─tld/
 │         │  └─web.xml
 │         ├─index.html
 │         └─result.jsp
 └─pom.xml
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Wed, 15 Jul 2015 00:00:00 +0200</pubDate>
				<link>/web%20development/2015/07/15/Tomcat+Intellj+OpenShift.html</link>
				<guid isPermaLink="true">/web%20development/2015/07/15/Tomcat+Intellj+OpenShift.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (3)</title>
				<description>&lt;h2 id=&quot;normal-equation&quot;&gt;Normal equation&lt;/h2&gt;
&lt;p&gt;Apart from the gradient descent, normal equation is another method to find the minimum J analytically. From the last part, we know the cost function and the input x for one example of n features&lt;/p&gt;

&lt;p&gt;\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \]&lt;/p&gt;

&lt;p&gt;\[x^{(i)} =  \begin{vmatrix}
x^{(i)}_0 \\
x^{(i)}_1 \\
x^{(i)}_2 \\
. \\
. \\
. \\
x^{(i)}_n
\end{vmatrix} \quad \text{where } x_0=1 \]&lt;/p&gt;

&lt;p&gt;Then we got a &lt;strong&gt;design matrix&lt;/strong&gt; that is an m by (n+1) dimensional matrix&lt;/p&gt;

&lt;p&gt;\[X =  \begin{vmatrix}
… &amp;amp; ^tx^{(1)} &amp;amp; … \\
… &amp;amp; ^tx^{(2)} &amp;amp; … \\
… &amp;amp; ^tx^{(3)} &amp;amp; … \\
&amp;amp;. \\
&amp;amp;. \\
&amp;amp;. \\
… &amp;amp; ^tx^{(m)} &amp;amp; … 
\end{vmatrix} \]&lt;/p&gt;

&lt;p&gt;Whereas y is a vector of dimension m&lt;/p&gt;

&lt;p&gt;\[y =  \begin{vmatrix}
y^{(1)} \\
y^{(2)} \\
y^{(3)} \\
. \\
. \\
. \\
y^{(m)}
\end{vmatrix} \]&lt;/p&gt;

&lt;p&gt;To minimize the cost function, we calculate the &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; from&lt;/p&gt;

&lt;p&gt;\[
\boxed{\theta=(X^TX)^{-1}X^Ty}
\]&lt;/p&gt;

&lt;h3 id=&quot;characteristics&quot;&gt;Characteristics&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Advantages
    &lt;ul&gt;
      &lt;li&gt;no need to do the feature scaling&lt;/li&gt;
      &lt;li&gt;no need to choose the learning rate&lt;/li&gt;
      &lt;li&gt;no need to iterate&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Disadvantage
    &lt;ul&gt;
      &lt;li&gt;The calculation of the &lt;script type=&quot;math/tex&quot;&gt;(X^TX)^{-1}&lt;/script&gt; could be very slow if n is large (&amp;gt;10000) since the complexity for a matrix n by n is &lt;script type=&quot;math/tex&quot;&gt;O(n^3)&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;(X^TX)&lt;/script&gt; is non-invertible (singular / degenerate), it could be caused by 2 reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Redundant feature, which leads to linear dependences&lt;/li&gt;
  &lt;li&gt;Too many features (&lt;script type=&quot;math/tex&quot;&gt;m \leq n&lt;/script&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the second case, we can delete some features or use the &lt;strong&gt;regularization&lt;/strong&gt; that we’ll talk about later.&lt;/p&gt;
</description>
				<pubDate>Tue, 14 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/14/ML-Notes-3.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/14/ML-Notes-3.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (2)</title>
				<description>&lt;p&gt;Having studied the univariable linear regression, we are going to extend the dimension from 1 to n. n is the number of the features that result in an output.&lt;/p&gt;

&lt;h2 id=&quot;multiple-variables&quot;&gt;Multiple variables&lt;/h2&gt;
&lt;p&gt;For a multivatiate linear regression, we take&lt;/p&gt;

&lt;p&gt;\[x =  \begin{vmatrix}
x_0 \\
x_1 \\
x_2 \\
. \\
. \\
. \\
x_n
\end{vmatrix} \quad \text{where } x_0=1 \]&lt;/p&gt;

&lt;p&gt;\[\theta^T =  \begin{vmatrix}
\theta_0 &amp;amp;  \theta_1 &amp;amp; \theta_2 &amp;amp; … &amp;amp; \theta_n
\end{vmatrix} \]&lt;/p&gt;

&lt;p&gt;The hypothesis function is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{\theta}(x)=\theta^Tx&lt;/script&gt;

&lt;p&gt;Given the cost function&lt;/p&gt;

&lt;p&gt;\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \]&lt;/p&gt;

&lt;p&gt;we could deduce the gradient descent&lt;/p&gt;

&lt;p&gt;\[
 \theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j} = \theta_j-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}) 
\]&lt;/p&gt;

&lt;h2 id=&quot;feature-scaling&quot;&gt;Feature scaling&lt;/h2&gt;
&lt;p&gt;The goal of feature scaling is to make sure the features are on a similar scale. It could accelerate the convergence&lt;br /&gt;
&lt;strong&gt;Mean normalization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;\[
  x_j:=\frac{x_j-\mu_j}{(max - min)}\quad \text{so} -0.5 \leq x_j \leq 0.5
  \]&lt;/p&gt;

&lt;h2 id=&quot;tricks-for-the-debugging&quot;&gt;Tricks for the debugging&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;To make sure the gradient descent working correctly, with an increasing number of iteration, the J should decrease&lt;/li&gt;
  &lt;li&gt;Choose the learning rate 0.001 -&amp;gt; 0.003 -&amp;gt; 0.01 -&amp;gt; 0.03 -&amp;gt; 0.1 -&amp;gt; 0.3 -&amp;gt; 1&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Polynomial regression
&lt;script type=&quot;math/tex&quot;&gt;h_{\theta}(x)=\theta_0+\theta_1x+\theta_2x^2=\theta_0+\theta_1x_1+\theta_2x_2&lt;/script&gt;
In this case, the feature scale becomes more important.
However, the square function will finally decrease with a great x. To avoid that, we could choose another model&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{\theta}(x)=\theta_0+\theta_1x+\theta_2\sqrt{x}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 14 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/14/ML-Notes-2.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/14/ML-Notes-2.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (1)</title>
				<description>&lt;p&gt;This is my personal note while watching the machine learning course on &lt;a href=&quot;https://www.coursera.org&quot; title=&quot;Coursera&quot;&gt;Coursera&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introduction-of-machine-learning&quot;&gt;Introduction of Machine Learning&lt;/h2&gt;
&lt;p&gt;The machine learning could be divided into 2 parts&lt;/p&gt;

&lt;h3 id=&quot;supervised-learning&quot;&gt;Supervised Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Regression&lt;/li&gt;
  &lt;li&gt;Classification (labeled)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;unsupervised-learning&quot;&gt;Unsupervised Learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Clustering the data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;univariable-linear-regression&quot;&gt;Univariable linear regression&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hypothesis function&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{\theta}(x)=\theta_0 + \theta_1 x&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cost function&lt;br /&gt;
The cost function for univariable linear regression is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_function&quot;&gt;convex function&lt;/a&gt; (or called bowl-shaped function). Therefore, it only has one global minimum point&lt;/p&gt;

    &lt;p&gt;\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient descend&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is the learning rate that can’t be too big or too small. A big rate could lead to a divergence instead of a convergence we expect. And a tiny rate would make the process too slow.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j}&lt;/script&gt;

    &lt;p&gt;Therefore we can conclude that&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(\theta_0 + \theta_1 x^{(i)} -y^{(i)}))^2&lt;/script&gt;

    &lt;p&gt;Deriving the function, we got&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[\begin{aligned}
   \theta_0:=\theta_0-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})) \\
 \theta_1:=\theta_1-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}) 
 \end{aligned} \]&lt;/p&gt;

&lt;p&gt;m - the size of the training examples&lt;br /&gt;
  x - input&lt;br /&gt;
  y - output&lt;br /&gt;
  This method is called &lt;strong&gt;Batch Gradient Descent&lt;/strong&gt; when we use all the training examples&lt;/p&gt;
</description>
				<pubDate>Sun, 12 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/12/ML-Notes-1.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/12/ML-Notes-1.html</guid>
			</item>
		
	</channel>
</rss>
